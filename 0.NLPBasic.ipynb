{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLP基本操作\n",
    "\n",
    "# 安装包：\n",
    "# jieba + spacy + sklearn + requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取目录下所有文件的路径: ['/Users/fubin/Downloads/复习/NLP/src/0.Basic.ipynb']\n",
      "分词(全模式)： ['结婚', '的', '和尚', '尚未', '未结', '结婚', '的确', '确实', '实在', '干扰', '分词', '啊']\n",
      "分词(精确模式)： ['结婚', '的', '和', '尚未', '结婚', '的', '确实', '在', '干扰', '分词', '啊']\n",
      "分词(搜索引擎模式)： ['结婚', '的', '和', '尚未', '结婚', '的', '确实', '在', '干扰', '分词', '啊']\n",
      "jieba.suggest_freq: ['高校', '网', '公序良俗']\n",
      "Counter: Counter({'的': 2, '结婚': 1, '和尚': 1, '未结婚': 1, '确实': 1, '在': 1, '干扰': 1, '分词': 1, '啊': 1})\n",
      "CountVectorizer: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] [[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "extract_tags: [pair('结婚', 'v'), pair('的', 'uj'), pair('和', 'c'), pair('尚未', 'd'), pair('结婚', 'v'), pair('的', 'uj'), pair('确实', 'ad'), pair('在', 'p'), pair('干扰', 'v'), pair('分词', 'n'), pair('啊', 'zg')]\n",
      "textrank: [('结婚', 1.0), ('干扰', 0.7583348675769241), ('分词', 0.4134021621233778)]\n",
      "TfidfVectorizer: ['document', 'second', 'second document', 'second second'] [[1.         0.         0.         0.        ]\n",
      " [0.25215917 0.79011212 0.39505606 0.39505606]\n",
      " [0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n<html lang=\"zh\">\n    <head>\n        <title>displaCy</title>\n    </head>\n\n    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n<figure style=\"margin-bottom: 6rem\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"zh\" id=\"fc1a87f90a6f45c8993afad0f0aa7ec7-0\" class=\"displacy\" width=\"1100\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">displaCy</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">uses</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">JavaScript,</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">SVG</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">and</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">CSS.</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 920.0,89.5 920.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-4\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-fc1a87f90a6f45c8993afad0f0aa7ec7-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n</g>\n</svg>\n</figure>\n</body>\n</html></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5001 ...\n",
      "Shutting down server on port 5001.\n"
     ]
    }
   ],
   "source": [
    "# 基本操作：读取文件、停用词、检测字符（数字、中文）、分词\n",
    "\n",
    "# spaCy是世界上最快的工业级自然语言处理工具。分词、词性标注、词干化、命名实体识别、名词短语提取\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 获取目录下所有文件的路径（遍历子文件夹）\n",
    "def get_all_files(dir):\n",
    "    files_ = []\n",
    "    list_ = os.listdir(dir)\n",
    "    for i in range(0, len(list_)):\n",
    "        path = os.path.join(dir, list_[i])\n",
    "        if os.path.isdir(path):\n",
    "            files_.extend(get_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            files_.append(os.path.abspath(path))\n",
    "    return files_\n",
    "print('获取目录下所有文件的路径:', get_all_files('.')[:1])\n",
    "\n",
    "# 获取停用词\n",
    "def read_stop_words(filepath):\n",
    "    return set(open(filepath, 'r',encoding='utf-8').read().split('\\n'))\n",
    "# 去停用词\n",
    "# stopwords =  read_stop_words('./stopwords.txt')\n",
    "# words = [word for word in ['i', 'love', 'you'] if word not in stopwords]\n",
    "        \n",
    "# 检测字符是否为中文\n",
    "def is_chinese(char):\n",
    "    if '\\u4e00' <= char <= '\\u9fff':\n",
    "        return True\n",
    "    return False\n",
    "# 检测字符是否为数字\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# 分句\n",
    "def cut_sentences(content):\n",
    "    # 结束符号，包含中文和英文的\n",
    "    end_flag = ['?', '!', '.', '？', '！', '。', '…']\n",
    "    content_len = len(content)\n",
    "    sentences = []\n",
    "    tmp_char = ''\n",
    "    for idx, char in enumerate(content):\n",
    "        # 拼接字符\n",
    "        tmp_char += char\n",
    "        # 判断是否已经到了最后一位\n",
    "        if (idx + 1) == content_len:\n",
    "            sentences.append(tmp_char)\n",
    "            break\n",
    "        # 判断此字符是否为结束符号\n",
    "        if char in end_flag:\n",
    "            # 再判断下一个字符是否为结束符号，如果不是结束符号，则切分句子\n",
    "            next_idx = idx + 1\n",
    "            if not content[next_idx] in end_flag:\n",
    "                sentences.append(tmp_char)\n",
    "                tmp_char = ''\n",
    "    return sentences\n",
    "\n",
    "# 分词\n",
    "sentence = \"结婚的和尚未结婚的确实在干扰分词啊\"\n",
    "print('分词(全模式)：', [token for token in jieba.cut(sentence, cut_all=True)]) # 全模式\n",
    "print('分词(精确模式)：', [token for token in jieba.cut(sentence, cut_all=False)]) # 精确模式\n",
    "print('分词(搜索引擎模式)：', [token for token in jieba.cut_for_search(sentence)]) # 搜索引擎模式\n",
    "\n",
    "# 分词加载新辞典\n",
    "# jieba.load_userdict(\"add_words_ch.txt\") #载入自定义词典，词典根据经验进行更新\n",
    "jieba.add_word(\"自定义词\")#只有一个词时可以直接这样加\n",
    "jieba.del_word(\"自定义词\") #也可以进行删除\n",
    "\n",
    "# 强制分开或和在一起分词\n",
    "jieba.suggest_freq((\"高校\",\"网\"),True) #这样可以让高校和网不被分到一起\n",
    "jieba.suggest_freq(\"公序良俗\",True) #这样可以让公序良俗分到一起\n",
    "print('jieba.suggest_freq:', jieba.lcut(\"高校网公序良俗\"))\n",
    "\n",
    "# 分词统计1\n",
    "from collections import Counter\n",
    "print('Counter:', Counter(['结婚', '的', '和尚', '未结婚', '的', '确实', '在', '干扰', '分词', '啊']))\n",
    "\n",
    "# 分词统计2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print ('CountVectorizer:', vectorizer.get_feature_names(), X.toarray())\n",
    "\n",
    "# 分词 + 词性标注\n",
    "from jieba import posseg\n",
    "print('extract_tags:', [a for a in posseg.cut('结婚的和尚未结婚的确实在干扰分词啊')])\n",
    "\n",
    "# 基于 TextRank 算法的关键词抽取\n",
    "from jieba import analyse\n",
    "print('textrank:', analyse.textrank('结婚的和尚未结婚的确实在干扰分词啊', topK=20, withWeight=True, allowPOS=('ns', 'n', 'vn', 'v')))\n",
    "\n",
    "# tfidf 词统计\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print ('TfidfVectorizer:', vectorizer.get_feature_names(), X.toarray())\n",
    "\n",
    "# 句法树\n",
    "# 自然语言处理库spaCy号称最快句法分析器\n",
    "# English\n",
    "# python3 -m spacy download en_core_web_sm --user\n",
    "# # Chinese\n",
    "# python3 -m spacy download zh_core_web_sm --user\n",
    "# 如果命令无法安装，自行下载文件然后解压进入文件夹执行python3 setup.py install\n",
    "# 该包放在./models/zh_core_web_sm-3.2.0.tar.gz\n",
    "import spacy\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "doc = nlp(u\"结婚 的 和尚 未结婚 的 确实 在 干扰 分词 啊\")\n",
    "spacy.displacy.serve(doc, style='dep', auto_select_port=True)\n",
    "\n",
    "# 词云可视化 (mac环境下无法安装)\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
    "# word_freq = {}\n",
    "# for c in corpus:\n",
    "#     for char in c.split(' '):\n",
    "#         word_freq[char] = word_freq[char]+1 if char in word_freq else 1\n",
    "# wc=WordCloud(font_path='/Users/fubin/Library/Fonts/SimHei.ttf').generate(word_freq)\n",
    "# plt.imshow(wc)\n",
    "# plt.axis('off')\n",
    "# plt.show();\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T12:49:02.543981Z",
     "start_time": "2023-08-30T12:46:22.465681Z"
    }
   },
   "id": "4ec15c00adbd91d0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA: [[ 0.97426394 -0.22541022]\n",
      " [ 0.51095028  0.85961027]\n",
      " [ 0.          0.        ]\n",
      " [ 0.97426394 -0.22541022]]\n",
      "LDA: [(0, 0.025000023), (1, 0.025000019), (2, 0.025000019), (3, 0.025000023), (4, 0.025000023), (5, 0.025000023), (6, 0.025000023), (7, 0.025000023), (8, 0.7749998), (9, 0.025000019)]\n",
      "word2vec: [ 0.03113786  0.05770715 -0.05098698  0.02809874 -0.02585673  0.00515335\n",
      "  0.05311637 -0.0278886   0.02823438 -0.0424185  -0.02217805  0.05874068\n",
      " -0.00986033  0.00200857 -0.02587894 -0.0480168 ]\n"
     ]
    }
   ],
   "source": [
    "# LSA / LDA / \n",
    "\n",
    "# LSA:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)\n",
    "svd_model = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=10)\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer),  ('svd', svd_model)])\n",
    "svd_matrix = svd_transformer.fit_transform(corpus)\n",
    "print('LSA:', svd_matrix)\n",
    "\n",
    "# LDA:\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import dictionary, Dictionary\n",
    "corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
    "data = [c.split(' ') for c in corpus]\n",
    "dictionary = Dictionary(data)\n",
    "dictionary.filter_n_most_frequent(3) # 过滤掉一些频率过高的词\n",
    "# dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(text) for text in data] # Bag-of-words representation of the documents.\n",
    "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10)\n",
    "topic_list=lda.print_topics(20)\n",
    "# print(topic_list)\n",
    "test_doc=data[2]\n",
    "doc_bow = dictionary.doc2bow(test_doc)      #文档转换成bow\n",
    "print('LDA:', lda[doc_bow])\n",
    "\n",
    "# word2vec:\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence'], ['this', 'is', 'the', 'second', 'sentence']]\n",
    "model = Word2Vec(sentences, vector_size=16, window=5, min_count=1, workers=4)\n",
    "model.save('./models/word2vec.model') # 保存模型\n",
    "model = Word2Vec.load('./models/word2vec.model') # 加载模型\n",
    "vector = model.wv['this']\n",
    "print('word2vec:', vector)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:07:22.937003Z",
     "start_time": "2023-08-30T13:07:22.917470Z"
    }
   },
   "id": "28bd87c6bc615e84"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77003it [00:02, 38208.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [37]\u001B[0m, in \u001B[0;36m<cell line: 17>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# 爬虫\u001B[39;00m\n\u001B[1;32m     16\u001B[0m imageset_already  \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(os\u001B[38;5;241m.\u001B[39mlistdir(image_out_dir)) \u001B[38;5;66;03m# icao24.jpg/... 其他后缀\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i,line \u001B[38;5;129;01min\u001B[39;00m tqdm(df_image\u001B[38;5;241m.\u001B[39miterrows()):\n\u001B[1;32m     18\u001B[0m     icao24, image_url \u001B[38;5;241m=\u001B[39m line[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124micao24\u001B[39m\u001B[38;5;124m'\u001B[39m], line[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m图片\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     19\u001B[0m     image_name \u001B[38;5;241m=\u001B[39m icao24\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mimage_url\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tqdm/std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:1324\u001B[0m, in \u001B[0;36mDataFrame.iterrows\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1322\u001B[0m klass \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor_sliced\n\u001B[1;32m   1323\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues):\n\u001B[0;32m-> 1324\u001B[0m     s \u001B[38;5;241m=\u001B[39m \u001B[43mklass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m k, s\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:461\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    459\u001B[0m NDFrame\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, data)\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n\u001B[0;32m--> 461\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfastpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:552\u001B[0m, in \u001B[0;36mSeries._set_axis\u001B[0;34m(self, axis, labels, fastpath)\u001B[0m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fastpath:\n\u001B[1;32m    550\u001B[0m     labels \u001B[38;5;241m=\u001B[39m ensure_index(labels)\n\u001B[0;32m--> 552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_is_all_dates\u001B[49m:\n\u001B[1;32m    553\u001B[0m     deep_labels \u001B[38;5;241m=\u001B[39m labels\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(labels, CategoricalIndex):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 爬虫\n",
    "\n",
    "# 1.6 爬取图片(for 基于多模态的智能机型识别)\n",
    "\n",
    "import os, requests, time\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df_image = pd.read_csv('/Users/fubin/Downloads/ADS-B研究/数据及其代码/metadata/opensky/militarty_icao24_all.csv')[['icao24','图片']]\n",
    "df_image = df_image.drop_duplicates(subset=['icao24','图片'], keep='first', ignore_index=True).dropna(axis=0)\n",
    "\n",
    "image_out_dir = '/Users/fubin/Downloads/ADS-B研究/数据及其代码/metadata/知识图谱/image/'\n",
    "print(df_image.shape[0])\n",
    "# 爬虫\n",
    "imageset_already  = set(os.listdir(image_out_dir)) # icao24.jpg/... 其他后缀\n",
    "for i,line in tqdm(df_image.iterrows()):\n",
    "    icao24, image_url = line['icao24'], line['图片']\n",
    "    image_name = icao24+'.'+image_url.split('.')[-1]\n",
    "    if image_name in imageset_already:\n",
    "        continue\n",
    "    img_data=requests.get(url=image_url).content\n",
    "    print(image_name)\n",
    "    # with open(image_out_dir + image_name,'wb') as fp:\n",
    "    #     fp.write(img_data)\n",
    "#     time.sleep(rd.randint(10,1000)/1000.)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T11:33:51.852397Z",
     "start_time": "2023-08-30T11:33:46.490518Z"
    }
   },
   "id": "e6a6fa6c41c8ca10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 文本分类: 未测试\n",
    "# fasttext很快，工业上常用\n",
    "\n",
    "import os\n",
    "import fasttext.FastText as fasttext\n",
    "classifier = fasttext.train_supervised('./data/file_path', label='__label__', dim=100, epoch=5, lr=0.01, wordNgrams=2, loss='softmax')\n",
    "classifier = fasttext.load_model(model)\n",
    "classifier.save_model(opt)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dceb55e89f81363f"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None None\n",
      "2 None 龚庄\n",
      "1 None None\n",
      "2 None 龚庄村\n",
      "1 None None\n",
      "2 None 龚庄村\n",
      "1 None 龚庄村\n",
      "2 None 一组\n",
      "[['龚庄']]\n",
      "[['龚庄村'], ['龚庄村', '一组']]\n",
      "False\n",
      "[[None, '龚庄'], [None, '龚庄村']]\n",
      "[[None, '龚庄'], [None, '龚庄村', '一组']]\n",
      "[[None, '龚庄'], [None, '龚庄村', '一组']]\n"
     ]
    }
   ],
   "source": [
    "# trie前缀树\n",
    "\n",
    "import os\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self, char = None):\n",
    "        self.char = char\n",
    "        self.children = {}\n",
    "        self.pv = 0\n",
    "        self.uv = 0\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "    def insert(self, char_list, pv_list, uv_list):\n",
    "        node = self.root\n",
    "        for i, char in enumerate(char_list):\n",
    "            if char in node.children:\n",
    "               node.children[char].pv += pv_list[i]\n",
    "               node.children[char].uv += uv_list[i]\n",
    "            else:\n",
    "                node.children[char] = TrieNode(char)\n",
    "                node.children[char].pv = pv_list[i]\n",
    "                node.children[char].uv = uv_list[i]\n",
    "            print(1, self.root.char, node.char)\n",
    "            node = node.children[char] # ?\n",
    "            print(2, self.root.char, node.char)\n",
    "    def search_next(self, char_list):\n",
    "        node = self.root\n",
    "        for char in char_list:\n",
    "            if char not in node.children: \n",
    "                return False\n",
    "            node = node.children[char]\n",
    "        seq_next = [char_list] + [char_list + [next_char] for next_char in node.children.keys()]\n",
    "        return seq_next\n",
    "    def print(self, max_deep_len):\n",
    "        def get_all_paths(node, len_):\n",
    "            if len_<=0 or len(node.children)==0:\n",
    "                return [[node.char]]\n",
    "            all_paths = []\n",
    "            for child_node in node.children.values():\n",
    "                paths = get_all_paths(child_node, len_-1)\n",
    "                for path in paths:\n",
    "                    all_paths.append([node.char] + path)\n",
    "            return all_paths\n",
    "        all_paths = get_all_paths(self.root, max_deep_len)\n",
    "        return all_paths\n",
    "if __name__ == \"__main__\":\n",
    "    trie = Trie()\n",
    "    trie.insert([\"龚庄\"],[1],[1])\n",
    "    trie.insert([\"龚庄村\"], [1], [1])\n",
    "    trie.insert([\"龚庄村\",\"一组\"], [1,1], [1,1])\n",
    "    \n",
    "    print(trie.search_next([\"龚庄\"]))\n",
    "    print(trie.search_next([\"龚庄村\"]))\n",
    "    print(trie.search_next([\"村\"]))\n",
    "    \n",
    "    print(trie.print(1))\n",
    "    print(trie.print(2))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T02:01:14.265893400Z",
     "start_time": "2023-08-31T02:01:14.234644Z"
    }
   },
   "id": "bf0b51ac95ee52b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abe2b1d5b3bdb550"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
